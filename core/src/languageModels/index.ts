/**
 * Get the content of a tag in a response from OpenAI.
 */
export function getTag({ tag, input }: TagRecord) {
  const regex = new RegExp(`<${tag}>(.*?)</${tag}>`, "is");
  const match = input.match(regex);
  return match ? match[1] : "";
}

/**
 * Stream the results of a chat completion
 */
export interface ChatCompletionStreamer {
  create: (opts: CreateChatCompletionParams) => Promise<{
    abortController: AbortController;
    stream: ChatStream;
  }>;
}

/**
 * Execute a language model programand get the resultsas a string
 */
export interface LanguageModelProgramExecutor {
  execute(records: ChatMessage[]): Promise<string>;
}

/**
 * The below is mostly taken directly from the OpenAI types, but the idea is to keep these
 * stable between different LLMs and between different OpenAI versions.
 */

export enum ChatMessageRoleEnum {
  System = "system",
  User = "user",
  Assistant = "assistant",
  Function = "function",
}

export interface ChatMessage {
  role: ChatMessageRoleEnum;
  content: string;
  name?: string;
  function_call?: FunctionCall;
}

type ChatStream = AsyncIterable<StreamingChatCompletionEvent>;

type TagRecord = {
  tag: string;
  input: string;
};

export interface FunctionSpecification {
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
   * underscores and dashes, with a maximum length of 64.
   */
  name: string;

  /**
   * The description of what the function does.
   */
  description?: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the
   * [guide](/docs/guides/gpt/function-calling) for examples, and the
   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
   * documentation about the format.
   */
  parameters?: Record<string, unknown>;
}

export interface FunctionCall {
  /**
   * The arguments to call the function with, as generated by the model in JSON
   * format. Note that the model does not always generate valid JSON, and may
   * hallucinate parameters not defined by your function schema. Validate the
   * arguments in your code before calling your function.
   */
  arguments?: string;

  /**
   * The name of the function to call.
   */
  name?: string;
}

export interface StreamingDelta {
  /**
   * The contents of the chunk message.
   */
  content?: string | null;

  /**
   * The name and arguments of a function that should be called, as generated by the
   * model.
   */
  function_call?: FunctionCall;

  /**
   * The role of the author of this message.
   */
  role?: "system" | "user" | "assistant" | "function";
}

export interface StreamingChatCompletionEventChoices {
  delta?: StreamingDelta;

  finish_reason?: "stop" | "length" | "function_call";

  index?: number;
}

export interface StreamingChatCompletionEvent {
  choices: StreamingChatCompletionEventChoices[];

  created: number;

  id: string;

  model: string;

  object: string;
}

export interface FunctionSpecification {
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
   * underscores and dashes, with a maximum length of 64.
   */
  name: string;

  /**
   * The description of what the function does.
   */
  description?: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the
   * [guide](/docs/guides/gpt/function-calling) for examples, and the
   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
   * documentation about the format.
   */
  parameters?: Record<string, unknown>;
}

export interface CreateChatCompletionParams {
  /**
   * A list of messages comprising the conversation so far.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
   */
  messages: ChatMessage[];

  /**
   * ID of the model to use. See the
   * [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table
   * for details on which models work with the Chat API.
   */
  // model: string;

  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be
   * sent as data-only
   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
   * as they become available, with the stream terminated by a `data: [DONE]`
   * message.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
   */
  // stream: boolean;

  // /**
  //  * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
  //  * existing frequency in the text so far, decreasing the model's likelihood to
  //  * repeat the same line verbatim.
  //  *
  //  * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
  //  */
  // frequency_penalty?: number | null;

  /**
   * Controls how the model responds to function calls. "none" means the model does
   * not call a function, and responds to the end-user. "auto" means the model can
   * pick between an end-user or calling a function. Specifying a particular function
   * via `{"name":\ "my_function"}` forces the model to call that function. "none" is
   * the default when no functions are present. "auto" is the default if functions
   * are present.
   */
  function_call?: "none" | "auto" | (FunctionCall & { name: string });

  /**
   * A list of functions the model may generate JSON inputs for.
   */
  functions?: FunctionSpecification[];

  // /**
  //  * Modify the likelihood of specified tokens appearing in the completion.
  //  *
  //  * Accepts a json object that maps tokens (specified by their token ID in the
  //  * tokenizer) to an associated bias value from -100 to 100. Mathematically, the
  //  * bias is added to the logits generated by the model prior to sampling. The exact
  //  * effect will vary per model, but values between -1 and 1 should decrease or
  //  * increase likelihood of selection; values like -100 or 100 should result in a ban
  //  * or exclusive selection of the relevant token.
  //  */
  // logit_bias?: unknown | null;

  /**
   * The maximum number of [tokens](/tokenizer) to generate in the chat completion.
   *
   * The total length of input tokens and generated tokens is limited by the model's
   * context length.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
   * for counting tokens.
   */
  max_tokens?: number;

  // /**
  //  * How many chat completion choices to generate for each input message.
  //  */
  // n?: number | null;

  // /**
  //  * Number between -2.0 and 2.0. Positive values penalize new tokens based on
  //  * whether they appear in the text so far, increasing the model's likelihood to
  //  * talk about new topics.
  //  *
  //  * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
  //  */
  // presence_penalty?: number | null;

  // /**
  //  * Up to 4 sequences where the API will stop generating further tokens.
  //  */
  // stop?: string | null | Array<string>;

  // /**
  //  * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
  //  * make the output more random, while lower values like 0.2 will make it more
  //  * focused and deterministic.
  //  *
  //  * We generally recommend altering this or `top_p` but not both.
  //  */
  // temperature?: number | null;

  // /**
  //  * An alternative to sampling with temperature, called nucleus sampling, where the
  //  * model considers the results of the tokens with top_p probability mass. So 0.1
  //  * means only the tokens comprising the top 10% probability mass are considered.
  //  *
  //  * We generally recommend altering this or `temperature` but not both.
  //  */
  // top_p?: number | null;

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor
   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
   */
  user?: string;
}
